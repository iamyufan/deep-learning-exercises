{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import DCGAN, WGAN\n",
    "\n",
    "\n",
    "# Load the FashionMNIST dataset\n",
    "dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan_1 = DCGAN(\n",
    "    latent_dim=100,\n",
    "    num_classes=10,\n",
    "    img_channels=1,\n",
    "    learning_rate=0.0002,\n",
    "    beta1=0.2,\n",
    "    num_layers_G=4,\n",
    "    num_layers_D=4,\n",
    "    nonlinearity_G=\"ReLU\",\n",
    "    nonlinearity_D=\"LeakyReLU\",\n",
    ")\n",
    "print(dcgan_1.netG)\n",
    "print(dcgan_1.netD)\n",
    "print(\"===============================================\")\n",
    "\n",
    "\n",
    "G_losses, D_losses = dcgan_1.train(\n",
    "    dataloader,\n",
    "    num_epochs=10,\n",
    "    output_dir=\"./output/dcgan_1\",\n",
    "    checkpoint_dir=\"./checkpoints/dcgan_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (label_emb): Embedding(10, 10)\n",
      "  (main): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(110, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (label_emb): Embedding(10, 10)\n",
      "  (main): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(11, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Identity()\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Identity()\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Identity()\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n",
      "===============================================\n",
      "Training the model on mps\n",
      "Saving output images to ./output/wgan_1\n",
      "Saving model checkpoints to ./checkpoints/wgan_1\n",
      "Epoch [1/10] Batch [0/235]\tLoss_D: -0.9921 \t Loss_G: -0.0002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(wgan_1\u001b[38;5;241m.\u001b[39mnetD)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m G_losses, D_losses \u001b[38;5;241m=\u001b[39m \u001b[43mwgan_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./output/wgan_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoints/wgan_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CornellTech/courses/24Fall/CS5787-DL/deep-learning-exercises/ex3/models/gan.py:61\u001b[0m, in \u001b[0;36mGAN.train\u001b[0;34m(self, dataloader, num_epochs, output_dir, checkpoint_dir)\u001b[0m\n\u001b[1;32m     58\u001b[0m D_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     62\u001b[0m         errG, errD \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(batch, i)\n\u001b[1;32m     64\u001b[0m         G_losses\u001b[38;5;241m.\u001b[39mappend(errG\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:629\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:672\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    671\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    674\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torchvision/datasets/mnist.py:143\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    139\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/PIL/Image.py:3154\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3152\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/PIL/Image.py:3016\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3012\u001b[0m         im\u001b[38;5;241m.\u001b[39mfrombytes(data, decoder_name, args)\n\u001b[1;32m   3013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 3016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrombuffer\u001b[39m(mode, size, data, decoder_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;124;03m    Creates an image memory referencing pixel data in a byte buffer.\u001b[39;00m\n\u001b[1;32m   3019\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.1.4\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3050\u001b[0m     _check_size(size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wgan_1 = WGAN(\n",
    "    latent_dim=100,\n",
    "    num_classes=10,\n",
    "    img_channels=1,\n",
    "    learning_rate=0.0002,\n",
    "    num_layers_G=4,\n",
    "    num_layers_D=4,\n",
    "    nonlinearity_G=\"ReLU\",\n",
    "    nonlinearity_D=\"LeakyReLU\",\n",
    "    norm_layer_D=\"none\",\n",
    ")\n",
    "print(wgan_1.netG)\n",
    "print(wgan_1.netD)\n",
    "print(\"===============================================\")\n",
    "\n",
    "\n",
    "G_losses, D_losses = wgan_1.train(\n",
    "    dataloader,\n",
    "    num_epochs=10,\n",
    "    output_dir=\"./output/wgan_1\",\n",
    "    checkpoint_dir=\"./checkpoints/wgan_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:06<00:00, 4078926.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 312659.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:01<00:00, 2238688.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 7302088.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the FashionMNIST dataset\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from models.components import Generator, Discriminator\n",
    "from models.gan import GAN\n",
    "\n",
    "\n",
    "class DCGAN(GAN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        learning_rate=0.0002,\n",
    "        beta1=0.2,\n",
    "        num_layers_G=4,\n",
    "        num_layers_D=4,\n",
    "        nonlinearity_G=\"ReLU\",\n",
    "        nonlinearity_D=\"LeakyReLU\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The DCGAN class that combines the Generator and Discriminator.\n",
    "        Follows the PyTorch Lightning Module structure that wraps the training loop.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        learning_rate : float\n",
    "            The learning rate for the optimizer.\n",
    "        beta1 : float\n",
    "            The beta1 parameter for the Adam optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            latent_dim=latent_dim,\n",
    "            num_classes=num_classes,\n",
    "            img_channels=img_channels,\n",
    "        )\n",
    "        # Generator and Discriminator\n",
    "        self.netG = Generator(\n",
    "            latent_dim=latent_dim,\n",
    "            num_classes=num_classes,\n",
    "            img_channels=img_channels,\n",
    "            num_layers=num_layers_G,\n",
    "            nonlinearity=nonlinearity_G,\n",
    "        ).to(self.device)\n",
    "        self.netD = Discriminator(\n",
    "            num_classes=num_classes,\n",
    "            img_channels=img_channels,\n",
    "            num_layers=num_layers_D,\n",
    "            nonlinearity=nonlinearity_D,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Training configurations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "\n",
    "    @property\n",
    "    def criterion(self):\n",
    "        if not hasattr(self, \"_criterion\"):\n",
    "            self._criterion = nn.BCELoss()\n",
    "        return self._criterion\n",
    "\n",
    "    @property\n",
    "    def optimizerG(self):\n",
    "        if not hasattr(self, \"_optimizerG\"):\n",
    "            self._optimizerG = optim.Adam(\n",
    "                self.netG.parameters(), lr=self.learning_rate, betas=(self.beta1, 0.999)\n",
    "            )\n",
    "        return self._optimizerG\n",
    "\n",
    "    @property\n",
    "    def optimizerD(self):\n",
    "        if not hasattr(self, \"_optimizerD\"):\n",
    "            self._optimizerD = optim.Adam(\n",
    "                self.netD.parameters(), lr=self.learning_rate, betas=(self.beta1, 0.999)\n",
    "            )\n",
    "        return self._optimizerD\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: Tuple[torch.Tensor],\n",
    "        batch_idx: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run a single training step on a batch of data and\n",
    "        return the losses of the Generator and Discriminator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Tuple[torch.Tensor]\n",
    "            A tuple containing\n",
    "            - input images (batch_size, img_channels, 64, 64) and\n",
    "            - labels (batch_size).\n",
    "        batch_idx : int\n",
    "            The index of the current batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        errG : torch.Tensor\n",
    "            The loss of the Generator.\n",
    "        errD : torch.Tensor\n",
    "            The loss of the Discriminator.\n",
    "        \"\"\"\n",
    "        real_images, class_labels = batch\n",
    "        real_images = real_images.to(self.device)\n",
    "        class_labels = class_labels.to(self.device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Labels for real and fake data\n",
    "        real_labels = torch.ones(batch_size, device=self.device)\n",
    "        fake_labels = torch.zeros(batch_size, device=self.device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        self.netD.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        output_real = self.netD(real_images, class_labels).view(-1)\n",
    "        print(output_real.shape)\n",
    "        print(real_labels.shape)\n",
    "        errD_real = self.criterion(output_real, real_labels)\n",
    "\n",
    "        # Fake images\n",
    "        noise = torch.randn(batch_size, self.latent_dim, 1, 1, device=self.device)\n",
    "        fake_images = self.netG(noise, class_labels)\n",
    "        output_fake = self.netD(fake_images.detach(), class_labels).view(-1)\n",
    "        errD_fake = self.criterion(output_fake, fake_labels)\n",
    "\n",
    "        # Total Discriminator loss\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        self.optimizerD.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        self.netG.zero_grad()\n",
    "        output_fake = self.netD(fake_images, class_labels).view(-1)\n",
    "        errG = self.criterion(output_fake, real_labels)\n",
    "        errG.backward()\n",
    "        self.optimizerG.step()\n",
    "\n",
    "        return errG, errD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (label_emb): Embedding(10, 10)\n",
      "  (main): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(110, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (label_emb): Embedding(10, 10)\n",
      "  (main): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(11, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dcgan_1 = DCGAN(\n",
    "    latent_dim=100,\n",
    "    num_classes=10,\n",
    "    img_channels=1,\n",
    "    learning_rate=0.0002,\n",
    "    beta1=0.2,\n",
    "    num_layers_G=4,\n",
    "    num_layers_D=4,\n",
    "    nonlinearity_G=\"ReLU\",\n",
    "    nonlinearity_D=\"LeakyReLU\",\n",
    ")\n",
    "print(dcgan_1.netG)\n",
    "print(dcgan_1.netD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on cpu\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdcgan_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./output/dcgan_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CornellTech/courses/24Fall/CS5787-DL/deep-learning-exercises/ex3/models/gan.py:44\u001b[0m, in \u001b[0;36mGAN.train\u001b[0;34m(self, dataloader, num_epochs, output_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m---> 44\u001b[0m         errG, errD \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     48\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Batch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss_D: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrD\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_G: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrG\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m             )\n",
      "Cell \u001b[0;32mIn[85], line 135\u001b[0m, in \u001b[0;36mDCGAN.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    133\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    134\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetG(noise, class_labels)\n\u001b[0;32m--> 135\u001b[0m output_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m errD_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output_fake, fake_labels)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Total Discriminator loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CornellTech/courses/24Fall/CS5787-DL/deep-learning-exercises/ex3/models/components.py:414\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, img, labels)\u001b[0m\n\u001b[1;32m    408\u001b[0m label_embedding \u001b[38;5;241m=\u001b[39m label_embedding\u001b[38;5;241m.\u001b[39mexpand(\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    410\u001b[0m )  \u001b[38;5;66;03m# (batch_size, num_classes, 64, 64)\u001b[39;00m\n\u001b[1;32m    411\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    412\u001b[0m     [img, label_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    413\u001b[0m )  \u001b[38;5;66;03m# (batch_size, img_channels + num_classes, 64, 64)\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dcgan_1.train(dataloader, num_epochs=10, output_dir=\"./output/dcgan_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-layer G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Function to select nonlinearity\n",
    "def get_nonlinearity(name):\n",
    "    if name == \"ReLU\":\n",
    "        return nn.ReLU(True)\n",
    "    elif name == \"LeakyReLU\":\n",
    "        return nn.LeakyReLU(0.2, inplace=True)\n",
    "    elif name == \"softplus\":\n",
    "        return lambda x: (torch.nn.functional.softplus(2 * x + 2) / 2) - 1\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid nonlinearity selection.\")\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        num_layers=4,\n",
    "        nonlinearity=\"ReLU\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        num_layers : int\n",
    "            The number of layers in the Generator, not including the output layer.\n",
    "        nonlinearity : str\n",
    "            The nonlinearity to use in the Generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.input_dim = latent_dim + num_classes\n",
    "\n",
    "        # The embedding layer for the class labels\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        # Get the main body of the Generator\n",
    "        if num_layers == 4:\n",
    "            main = self._get_4_layer()\n",
    "        elif num_layers == 8:\n",
    "            main = self._get_8_layer()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid number of layers for Generator.\")\n",
    "\n",
    "        # Output layer: produces 1-channel 64x64 image in [-1, 1]\n",
    "        main.append(\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n",
    "        )\n",
    "        main.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "    def _get_4_layer(self):\n",
    "        return [\n",
    "            # Layer 1: (1x1) -> (4x4)\n",
    "            nn.ConvTranspose2d(self.input_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            get_nonlinearity(self.nonlinearity),\n",
    "            # Layer 2: (4x4) -> (8x8)\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            get_nonlinearity(self.nonlinearity),\n",
    "            # Layer 3: (8x8) -> (16x16)\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            get_nonlinearity(self.nonlinearity),\n",
    "            # Layer 4: (16x16) -> (32x32)\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            get_nonlinearity(self.nonlinearity),\n",
    "        ]\n",
    "\n",
    "    def _get_8_layer(self):\n",
    "        return [\n",
    "            # Layer 1: (1x1) -> (4x4)\n",
    "            nn.ConvTranspose2d(self.input_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 2: (4x4) -> (4x4)\n",
    "            nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 3: (4x4) -> (8x8)\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 4: (8x8) -> (8x8)\n",
    "            nn.ConvTranspose2d(256, 256, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 5: (8x8) -> (16x16)\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 6: (16x16) -> (16x16)\n",
    "            nn.ConvTranspose2d(128, 128, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 7: (16x16) -> (32x32)\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 8: (32x32) -> (32x32)\n",
    "            nn.ConvTranspose2d(64, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "        ]\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_noise = torch.randn(2, 100, 1, 1)\n",
    "sample_labels = torch.randint(0, 10, (2,))\n",
    "sample_images = torch.randn(2, 1, 64, 64)\n",
    "\n",
    "gen = Generator(num_layers=8, nonlinearity=\"LeakyReLU\")\n",
    "\n",
    "gen(sample_noise, sample_labels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is the latent vector z + class label, going into a transposed conv layer\n",
    "            nn.ConvTranspose2d(latent_dim + num_classes, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 2\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 3\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 4\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Output layer: produces 1-channel 64x64 image\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),  # Output range should be [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Discriminator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is the image + class label, going into a conv layer\n",
    "            nn.Conv2d(img_channels + num_classes, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Output layer\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),  # Output probability between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator to classify real/fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The input images to be classified.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, 1, 1, 1)\n",
    "            The probability of the input images being real.\n",
    "        \"\"\"\n",
    "        # Concatenate image and class label embedding\n",
    "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        label_embedding = label_embedding.expand(-1, -1, img.size(2), img.size(3))\n",
    "        img = torch.cat([img, label_embedding], dim=1)\n",
    "        return self.main(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 64, 64]), torch.Size([2, 1, 1, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_noise = torch.randn(2, 100, 1, 1)\n",
    "sample_labels = torch.randint(0, 10, (2,))\n",
    "sample_images = torch.randn(2, 1, 64, 64)\n",
    "\n",
    "gen = Generator()\n",
    "disc = Discriminator()\n",
    "\n",
    "gen(sample_noise, sample_labels).shape, disc(sample_images, sample_labels).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-layer G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN with 8 layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the output images.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.ConvTranspose2d(latent_dim + num_classes, 512, 4, 1, 0, bias=False),  # (1x1) -> (4x4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 2\n",
    "            nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),  # (4x4) -> (4x4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 3\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),  # (4x4) -> (8x8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 4\n",
    "            nn.ConvTranspose2d(256, 256, 3, 1, 1, bias=False),  # (8x8) -> (8x8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 5\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # (8x8) -> (16x16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 6\n",
    "            nn.ConvTranspose2d(128, 128, 3, 1, 1, bias=False),  # (16x16) -> (16x16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 7\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),   # (16x16) -> (32x32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 8\n",
    "            nn.ConvTranspose2d(64, 64, 3, 1, 1, bias=False),    # (32x32) -> (32x32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Output Layer\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),  # (32x32) -> (64x64)\n",
    "            nn.Tanh(),  # Output range should be [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Discriminator class for DCGAN with 8 layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(img_channels + num_classes, 64, 4, 2, 1, bias=False),  # (64x64) -> (32x32)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=False),  # (32x32) -> (32x32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),  # (32x32) -> (16x16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 128, 3, 1, 1, bias=False),  # (16x16) -> (16x16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 5\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),  # (16x16) -> (8x8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 6\n",
    "            nn.Conv2d(256, 256, 3, 1, 1, bias=False),  # (8x8) -> (8x8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 7\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),  # (8x8) -> (4x4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 8\n",
    "            nn.Conv2d(512, 512, 3, 1, 1, bias=False),  # (4x4) -> (4x4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Output Layer\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),  # (4x4) -> (1x1)\n",
    "            nn.Sigmoid(),  # Output probability between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator to classify real/fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The input images to be classified.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, 1, 1, 1)\n",
    "            The probability of the input images being real.\n",
    "        \"\"\"\n",
    "        # Concatenate image and class label embedding\n",
    "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        label_embedding = label_embedding.expand(-1, -1, img.size(2), img.size(3))\n",
    "        img = torch.cat([img, label_embedding], dim=1)\n",
    "        return self.main(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 64, 64]), torch.Size([2, 1, 1, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_noise = torch.randn(2, 100, 1, 1)\n",
    "sample_labels = torch.randint(0, 10, (2,))\n",
    "sample_images = torch.randn(2, 1, 64, 64)\n",
    "\n",
    "gen = Generator()\n",
    "disc = Discriminator()\n",
    "\n",
    "gen(sample_noise, sample_labels).shape, disc(sample_images, sample_labels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Function to select nonlinearity\n",
    "def get_nonlinearity(name):\n",
    "    if name == \"ReLU\":\n",
    "        return nn.ReLU(True)\n",
    "    elif name == \"LeakyReLU\":\n",
    "        return nn.LeakyReLU(0.2, inplace=True)\n",
    "    elif name == \"softplus\":\n",
    "        return lambda x: (torch.nn.functional.softplus(2 * x + 2) / 2) - 1\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid nonlinearity selection.\")\n",
    "\n",
    "\n",
    "def get_norm_layer(norm_layer, num_features, img_shape):\n",
    "    if norm_layer == \"bn\":\n",
    "        return nn.BatchNorm2d(num_features)\n",
    "    elif norm_layer == \"ln\":\n",
    "        return nn.LayerNorm([num_features, img_shape[0], img_shape[1]])\n",
    "    elif norm_layer == \"none\":\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization layer selection.\")\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        num_layers=4,\n",
    "        nonlinearity=\"ReLU\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN, WGAN, and WGAN-GP.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        num_layers : int\n",
    "            The number of layers in the Generator, not including the output layer.\n",
    "        nonlinearity : str\n",
    "            The nonlinearity to use in the Generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.input_dim = latent_dim + num_classes\n",
    "\n",
    "        # The embedding layer for the class labels\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        # Get the main body of the Generator\n",
    "        if num_layers == 4:\n",
    "            main = self._get_4_layer()\n",
    "        elif num_layers == 8:\n",
    "            main = self._get_8_layer()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid number of layers for Generator.\")\n",
    "\n",
    "        # Output layer: produces 1-channel 64x64 image in [-1, 1]\n",
    "        main.add_module(\n",
    "            \"output\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n",
    "                nn.Tanh(),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "    def _get_4_layer(self) -> nn.Sequential:\n",
    "        main = nn.Sequential()\n",
    "\n",
    "        # Layer 1: (1x1) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block1\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.input_dim, 512, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: (4x4) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block2\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: (8x8) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block3\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: (16x16) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block4\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return main\n",
    "\n",
    "    def _get_8_layer(self) -> nn.Sequential:\n",
    "        main = nn.Sequential()\n",
    "\n",
    "        # Layer 1: (1x1) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block1\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.input_dim, 512, 4, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: (4x4) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block2\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: (4x4) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block3\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: (8x8) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block4\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(256, 256, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 5: (8x8) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block5\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 6: (16x16) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block6\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 128, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 7: (16x16) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block7\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 8: (32x32) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block8\",\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(64, 64, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return main\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        num_layers=4,\n",
    "        nonlinearity=\"LeakyReLU\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Discriminator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        num_layers : int\n",
    "            The number of layers in the Discriminator, not including the output layer.\n",
    "        nonlinearity : str\n",
    "            The nonlinearity to use in the Discriminator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = img_channels + num_classes\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        # The embedding layer for the class labels\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        # Get the main body of the Discriminator\n",
    "        if num_layers == 4:\n",
    "            main = self._get_4_layer()\n",
    "        elif num_layers == 8:\n",
    "            main = self._get_8_layer()\n",
    "\n",
    "        # Output layer: produces probability of input image being real\n",
    "        main.append(\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        main.append(nn.Sigmoid())\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "    def _get_4_layer(self) -> nn.Sequential:\n",
    "        main = nn.Sequential()\n",
    "\n",
    "        # Layer 1: (64x64) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block1\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.input_dim, 64, 4, 2, 1, bias=False),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: (32x32) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block2\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: (16x16) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block3\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: (8x8) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block4\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return main\n",
    "\n",
    "    def _get_8_layer(self) -> nn.Sequential:\n",
    "        main = nn.Sequential()\n",
    "\n",
    "        # Layer 1: (64x64) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block1\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.input_dim, 64, 4, 2, 1, bias=False),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: (32x32) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block2\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 64, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: (32x32) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block3\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: (16x16) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block4\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 5: (16x16) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block5\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 6: (8x8) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block6\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 256, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 7: (8x8) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block7\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 8: (4x4) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block8\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 512, 3, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return main\n",
    "\n",
    "    def forward(self, img, labels) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator to classify real/fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The input images to be classified.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, 1, 1, 1)\n",
    "            The probability of the input images being real.\n",
    "        \"\"\"\n",
    "        # Concatenate image and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        label_embedding = label_embedding.expand(\n",
    "            -1, -1, img.size(2), img.size(3)\n",
    "        )  # (batch_size, num_classes, 64, 64)\n",
    "        img = torch.cat(\n",
    "            [img, label_embedding], dim=1\n",
    "        )  # (batch_size, img_channels + num_classes, 64, 64)\n",
    "        return self.main(img)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        num_layers=4,\n",
    "        nonlinearity=\"LeakyReLU\",\n",
    "        norm_layer=\"none\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Critic class for WGAN and WGAN-GP.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        num_layers : int\n",
    "            The number of layers in the Critic, not including the output layer.\n",
    "        nonlinearity : str\n",
    "            The nonlinearity to use in the Critic.\n",
    "        norm_layer : str\n",
    "            The normalization layer to use in the Critic.\n",
    "            \"bn\" for BatchNorm, \"ln\" for LayerNorm, and \"none\" for no normalization.\n",
    "            For WGAN, use \"none\". For WGAN-GP, use \"ln\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = img_channels + num_classes\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        # The embedding layer for the class labels\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        # Get the main body of the Discriminator\n",
    "        if num_layers == 4:\n",
    "            main = self._get_4_layer()\n",
    "        elif num_layers == 8:\n",
    "            main = self._get_8_layer()\n",
    "\n",
    "        # Output layer: produces probability of input image being real\n",
    "        main.append(\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        main.append(nn.Sigmoid())\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "    def _get_4_layer(self) -> nn.Sequential:\n",
    "        main = nn.Sequential()\n",
    "\n",
    "        # Layer 1: (64x64) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block1\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.input_dim, 64, 4, 2, 1, bias=False),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: (32x32) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block2\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 128, (16, 16)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: (16x16) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block3\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 256, (8, 8)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: (8x8) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block4\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 512, (4, 4)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return main\n",
    "\n",
    "    def _get_8_layer(self) -> nn.Sequential:\n",
    "        main = nn.Sequential()\n",
    "\n",
    "        # Layer 1: (64x64) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block1\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.input_dim, 64, 4, 2, 1, bias=False),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: (32x32) -> (32x32)\n",
    "        main.add_module(\n",
    "            \"block2\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 64, 3, 1, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 64, (32, 32)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: (32x32) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block3\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 128, (16, 16)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: (16x16) -> (16x16)\n",
    "        main.add_module(\n",
    "            \"block4\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 128, (16, 16)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 5: (16x16) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block5\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 256, (8, 8)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 6: (8x8) -> (8x8)\n",
    "        main.add_module(\n",
    "            \"block6\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 256, 3, 1, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 256, (8, 8)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 7: (8x8) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block7\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 512, (4, 4)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "        # Layer 8: (4x4) -> (4x4)\n",
    "        main.add_module(\n",
    "            \"block8\",\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 512, 3, 1, 1, bias=False),\n",
    "                get_norm_layer(self.norm_layer, 512, (4, 4)),\n",
    "                get_nonlinearity(self.nonlinearity),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return main\n",
    "\n",
    "    def forward(self, img, labels) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator to classify real/fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The input images to be classified.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, 1, 1, 1)\n",
    "            The probability of the input images being real.\n",
    "        \"\"\"\n",
    "        # Concatenate image and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        label_embedding = label_embedding.expand(\n",
    "            -1, -1, img.size(2), img.size(3)\n",
    "        )  # (batch_size, num_classes, 64, 64)\n",
    "        img = torch.cat(\n",
    "            [img, label_embedding], dim=1\n",
    "        )  # (batch_size, img_channels + num_classes, 64, 64)\n",
    "        return self.main(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 64, 64]),\n",
       " torch.Size([2, 1, 1, 1]),\n",
       " torch.Size([2, 1, 1, 1]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_noise = torch.randn(2, 100, 1, 1)\n",
    "sample_labels = torch.randint(0, 10, (2,))\n",
    "sample_images = torch.randn(2, 1, 64, 64)\n",
    "\n",
    "gen = Generator(num_layers=4, nonlinearity=\"LeakyReLU\")\n",
    "disc = Discriminator(num_layers=4, nonlinearity=\"LeakyReLU\")\n",
    "crit = Critic(num_layers=8, nonlinearity=\"LeakyReLU\", norm_layer=\"bn\")\n",
    "\n",
    "# Should output: (2, 1, 64, 64), (2, 1, 1, 1), (2, 1, 1, 1)\n",
    "gen(sample_noise, sample_labels).shape, disc(sample_images, sample_labels).shape, crit(sample_images, sample_labels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic(\n",
      "  (label_emb): Embedding(10, 10)\n",
      "  (main): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(11, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Identity()\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Identity()\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): Identity()\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Function to select nonlinearity\n",
    "def get_nonlinearity(name):\n",
    "    if name == \"ReLU\":\n",
    "        return nn.ReLU(True)\n",
    "    elif name == \"LeakyReLU\":\n",
    "        return nn.LeakyReLU(0.2, inplace=True)\n",
    "    elif name == \"softplus\":\n",
    "        return lambda x: (torch.nn.functional.softplus(2 * x + 2) / 2) - 1\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid nonlinearity selection.\")\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        nonlinearity=\"ReLU\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        nonlinearity : str\n",
    "            The nonlinearity to use in the Generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is the latent vector z + class label, going into a transposed conv layer\n",
    "            nn.ConvTranspose2d(latent_dim + num_classes, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            get_nonlinearity(nonlinearity),\n",
    "            # Layer 2\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            get_nonlinearity(nonlinearity),\n",
    "            # Layer 3\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            get_nonlinearity(nonlinearity),\n",
    "            # Layer 4\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            get_nonlinearity(nonlinearity),\n",
    "            # Output layer: produces 1-channel 64x64 image\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),  # Output range should be [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "\n",
    "sample_batch_image = torch.randn(1, 100, 1, 1)\n",
    "sample_batch_label = torch.randint(0, 10, (1,))\n",
    "output = generator(sample_batch_image, sample_batch_label)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN with 8 layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the output images.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.ConvTranspose2d(latent_dim + num_classes, 512, 4, 1, 0, bias=False),  # (1x1) -> (4x4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 2\n",
    "            nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),  # (4x4) -> (4x4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 3\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),  # (4x4) -> (8x8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 4\n",
    "            nn.ConvTranspose2d(256, 256, 3, 1, 1, bias=False),  # (8x8) -> (8x8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 5\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # (8x8) -> (16x16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 6\n",
    "            nn.ConvTranspose2d(128, 128, 3, 1, 1, bias=False),  # (16x16) -> (16x16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 7\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),   # (16x16) -> (32x32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 8\n",
    "            nn.ConvTranspose2d(64, 64, 3, 1, 1, bias=False),    # (32x32) -> (32x32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Output Layer\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),  # (32x32) -> (64x64)\n",
    "            nn.Tanh(),  # Output range should be [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "\n",
    "sample_batch_image = torch.randn(1, 100, 1, 1)\n",
    "sample_batch_label = torch.randint(0, 10, (1,))\n",
    "output = generator(sample_batch_image, sample_batch_label)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (13): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "base_filter_count = 64\n",
    "generator = build_generator(num_layers, base_filter_count)\n",
    "print(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvTranspose2d(100, 8192, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): ConvTranspose2d(8192, 4096, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): ConvTranspose2d(4096, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): ReLU(inplace=True)\n",
      "  (15): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (23): ReLU(inplace=True)\n",
      "  (24): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (25): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_layers = 8\n",
    "base_filter_count = 64\n",
    "generator = build_generator(num_layers, base_filter_count)\n",
    "print(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_batch = torch.randn(1, 100, 1, 1)\n",
    "output = generator(sample_batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Generator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is the latent vector z + class label, going into a transposed conv layer\n",
    "            nn.ConvTranspose2d(latent_dim + num_classes, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 2\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 3\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Layer 4\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Output layer: produces 1-channel 64x64 image\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),  # Output range should be [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        # Concatenate noise vector z and class label embedding\n",
    "        label_embedding = (\n",
    "            self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        )  # (batch_size, num_classes, 1, 1)\n",
    "        z = torch.cat(\n",
    "            [noise, label_embedding], dim=1\n",
    "        )  # (batch_size, latent_dim + num_classes, 1, 1)\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Discriminator class for DCGAN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is the image + class label, going into a conv layer\n",
    "            nn.Conv2d(img_channels + num_classes, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Output layer\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),  # Output probability between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator to classify real/fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The input images to be classified.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, 1, 1, 1)\n",
    "            The probability of the input images being real.\n",
    "        \"\"\"\n",
    "        # Concatenate image and class label embedding\n",
    "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        label_embedding = label_embedding.expand(-1, -1, img.size(2), img.size(3))\n",
    "        img = torch.cat([img, label_embedding], dim=1)\n",
    "        return self.main(img)\n",
    "\n",
    "\n",
    "class DCGAN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        num_classes=10,\n",
    "        img_channels=1,\n",
    "        learning_rate=0.0002,\n",
    "        beta1=0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The DCGAN class that combines the Generator and Discriminator.\n",
    "        Follows the PyTorch Lightning Module structure that wraps the training loop.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim : int\n",
    "            The dimension of the latent random noise vector.\n",
    "        num_classes : int\n",
    "            The number of classes in the dataset, used for label supervision.\n",
    "        img_channels : int\n",
    "            The number of channels in the input images.\n",
    "        learning_rate : float\n",
    "            The learning rate for the optimizer.\n",
    "        beta1 : float\n",
    "            The beta1 parameter for the Adam optimizer.\n",
    "        \"\"\"\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Configuration for the model\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        # Generator and Discriminator\n",
    "        self.netG = Generator(self.latent_dim, self.num_classes, self.img_channels).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.netD = Discriminator(self.num_classes, self.img_channels).to(self.device)\n",
    "\n",
    "        # Training configurations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "\n",
    "    @property\n",
    "    def criterion(self):\n",
    "        if not hasattr(self, \"_criterion\"):\n",
    "            self._criterion = nn.BCELoss()\n",
    "        return self._criterion\n",
    "\n",
    "    @property\n",
    "    def optimizerG(self):\n",
    "        if not hasattr(self, \"_optimizerG\"):\n",
    "            self._optimizerG = optim.Adam(\n",
    "                self.netG.parameters(), lr=self.learning_rate, betas=(self.beta1, 0.999)\n",
    "            )\n",
    "        return self._optimizerG\n",
    "\n",
    "    @property\n",
    "    def optimizerD(self):\n",
    "        if not hasattr(self, \"_optimizerD\"):\n",
    "            self._optimizerD = optim.Adam(\n",
    "                self.netD.parameters(), lr=self.learning_rate, betas=(self.beta1, 0.999)\n",
    "            )\n",
    "        return self._optimizerD\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator to generate fake images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise : torch.Tensor (batch_size, latent_dim, 1, 1)\n",
    "            The random noise vector sampled from a normal distribution.\n",
    "        labels : torch.Tensor (batch_size)\n",
    "            The class labels for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (batch_size, img_channels, 64, 64)\n",
    "            The generated fake images.\n",
    "        \"\"\"\n",
    "        return self.netG(noise, labels)\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: Tuple[torch.Tensor],\n",
    "        batch_idx: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run a single training step on a batch of data and\n",
    "        return the losses of the Generator and Discriminator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Tuple[torch.Tensor]\n",
    "            A tuple containing\n",
    "            - input images (batch_size, img_channels, 64, 64) and\n",
    "            - labels (batch_size).\n",
    "        batch_idx : int\n",
    "            The index of the current batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        errG : torch.Tensor\n",
    "            The loss of the Generator.\n",
    "        errD : torch.Tensor\n",
    "            The loss of the Discriminator.\n",
    "        \"\"\"\n",
    "        real_images, real_labels = batch\n",
    "        real_images = real_images.to(self.device)\n",
    "        real_labels = real_labels.to(self.device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        self.netD.zero_grad()\n",
    "\n",
    "        # Train on real images\n",
    "        label = torch.full((batch_size,), 1.0, dtype=torch.float).to(\n",
    "            self.device\n",
    "        )  # All ones label for real images\n",
    "        output = self.netD(real_images, real_labels).view(-1)\n",
    "        errD_real = self.criterion(output, label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # Train on fake images\n",
    "        noise = torch.randn(batch_size, self.latent_dim, 1, 1).to(self.device)\n",
    "        fake_labels = torch.randint(0, self.num_classes, (batch_size,)).to(self.device)\n",
    "        fake_images = self.netG(noise, fake_labels)\n",
    "        label.fill_(0.0)  # All zeros label for fake images\n",
    "        output = self.netD(fake_images.detach(), fake_labels).view(-1)\n",
    "        errD_fake = self.criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        self.optimizerD.step()\n",
    "\n",
    "        ## Train Generator ##\n",
    "        self.netG.zero_grad()\n",
    "        label.fill_(1.0)  # The generator wants to trick the discriminator\n",
    "\n",
    "        output = self.netD(fake_images, fake_labels).view(-1)\n",
    "        errG = self.criterion(output, label)\n",
    "        errG.backward()\n",
    "        self.optimizerG.step()\n",
    "\n",
    "        return errG, errD\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizerG, self.optimizerD], []\n",
    "\n",
    "    def generate_images_by_label(self, num_images: int, label: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate fake images in the format of torchvision grid\n",
    "        given a class label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_images : int\n",
    "            The number of images to generate.\n",
    "        label: int\n",
    "            The class label for the images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grid : torch.Tensor (3, H, W)\n",
    "            The grid of fake images.\n",
    "        \"\"\"\n",
    "        noise = torch.randn(num_images, self.latent_dim, 1, 1).to(self.device)\n",
    "        labels = torch.full((num_images,), label, dtype=torch.long).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_images = self.forward(noise, labels).detach().cpu()\n",
    "\n",
    "        grid = torchvision.utils.make_grid(fake_images, nrow=10, normalize=True)\n",
    "        return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
